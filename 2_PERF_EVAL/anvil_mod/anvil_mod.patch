diff --git a/anvil/anvil.c b/anvil/anvil.c
index 24d7549..ed7f4e4 100644
--- a/anvil/anvil.c
+++ b/anvil/anvil.c
@@ -1,55 +1,65 @@
-#define DEBUG
+// #define DEBUG
 
-#include <linux/kernel.h>
-#include <linux/module.h>
-#include <linux/perf_event.h>
-#include <linux/cpumask.h> 
-#include <linux/slab.h> 
-#include <linux/interrupt.h>
-#include <linux/workqueue.h>
+#include "anvil.h"
+
+#include <asm/page.h>
+#include <asm/uaccess.h>
+#include <linux/cpumask.h>
+#include <linux/delay.h>
+#include <linux/err.h>
+#include <linux/highmem.h>
 #include <linux/hrtimer.h>
+#include <linux/interrupt.h>
+#include <linux/kernel.h>
 #include <linux/ktime.h>
-#include <linux/err.h>
-#include <linux/percpu.h>
 #include <linux/mm.h>
-#include <linux/highmem.h>
+#include <linux/module.h>
+#include <linux/mutex.h>
+#include <linux/percpu.h>
+#include <linux/perf_event.h>
 #include <linux/sched.h>
-#include <asm/page.h>
-#include <asm/uaccess.h>
-#include <linux/delay.h>
+#include <linux/slab.h>
+#include <linux/workqueue.h>
+// #include <linux/semaphore.h>
+static DEFINE_MUTEX(get_user_mutex1);
+static DEFINE_MUTEX(get_user_mutex2);
+static DEFINE_MUTEX(pfn_to_page_mutex);
 
-#include "anvil.h"
+static DEFINE_MUTEX(disable_sampling);
+static DEFINE_MUTEX(enable_sampling);
+static DEFINE_MUTEX(count_sampling);
+// static DEFINE_MUTEX(llc_check);
+
+// static DECLARE_MUTEX(get_user_sem);
 
 #define MIN_SAMPLES 0
 #define REFRESHED_ROWS 1
 
-#define get_bank(page) ((page>>2)&7)^((page>>6)&7)
-
 MODULE_LICENSE("GPL");
 
 static struct hrtimer sample_timer;
 static ktime_t ktime;
-static u64 old_val,val;
-static u64 old_l1D_val,l1D_val,miss_total;
+static u64 old_val, val;
+static u64 old_ld_val, ld_val, miss_total;
+static u64 old_st_val, st_val;
 static sample_t sample_buffer[SAMPLES_MAX];
 static int sampling;
-static int start_sampling=0;
+static int start_sampling = 0;
 static int sample_head;
 static unsigned int sample_total;
 static profile_t profile[PROFILE_N];
 static unsigned int record_size;
 /* counts number of times L1 threhold was
 passed (sampling was done) */
-static unsigned long L1_count=0;
 /* counts number of times hammering was detected */
-static unsigned long L2_count=0;
-static unsigned long refresh_count=0;
+// static unsigned long L2_count = 0;
+// static unsigned long refresh_count = 0;
 static unsigned int hammer_threshold;
 unsigned long dummy;
 
 /* for logging */
-static struct sample_log log[25000];
-static int log_index=0;
+// static struct sample_log log[25000];
+// static int log_index = 0;
 
 static struct workqueue_struct *action_wq;
 static struct workqueue_struct *llc_event_wq;
@@ -59,535 +69,602 @@ static struct work_struct task2;
 static void sort(void);
 static void build_profile(void);
 DEFINE_PER_CPU(struct perf_event *, llc_event);
-DEFINE_PER_CPU(struct perf_event *, l1D_event);
+DEFINE_PER_CPU(struct perf_event *, ld_event);
+DEFINE_PER_CPU(struct perf_event *, st_event);
 DEFINE_PER_CPU(struct perf_event *, ld_lat_event);
 DEFINE_PER_CPU(struct perf_event *, precise_str_event);
 
-void action_wq_callback( struct work_struct *work);
-void llc_event_wq_callback( struct work_struct *work);
+void action_wq_callback(struct work_struct *work);
+void llc_event_wq_callback(struct work_struct *work);
 
-void llc_event_callback(struct perf_event *event,
-            struct perf_sample_data *data,
-            struct pt_regs *regs){}
+void llc_event_callback(struct perf_event *event, struct perf_sample_data *data,
+                        struct pt_regs *regs) {}
 
-void l1D_event_callback(struct perf_event *event,
-            struct perf_sample_data *data,
-            struct pt_regs *regs){}
+void ld_event_callback(struct perf_event *event, struct perf_sample_data *data,
+                       struct pt_regs *regs) {}
+void st_event_callback(struct perf_event *event, struct perf_sample_data *data,
+                       struct pt_regs *regs) {}
 
-/* returns a pfn of a page "inc" rows above the page "phy" in a base row */ 
+/* returns a pfn of a page "inc" rows above the page "phy" in a base row */
 /*@input: phy - physical page in base DRAM row
 @input: inc - offset to the base row
 
 @return: pfn of page in the row base row - inc
 */
 
-static unsigned long get_row_plus(unsigned long phy, int inc){
-	unsigned long bank_old = get_bank(phy);
-	unsigned long row_new = (phy>>6) + inc;
-	unsigned long bank_new = (row_new & 0x7) ^ bank_old;
-	unsigned long rank_new = (phy>>7)&1;
-
-	return (unsigned long)((row_new << 6) | (rank_new << 5) | (bank_new << 2) | (phy & 0x3));
-
+// 2RANK 1DIMM
+static unsigned long get_row_plus(unsigned long phy, int inc) {
+  unsigned long rank = (phy >> 1) & 0x1;
+
+  // unsigned long r3 = ((phy >> 9) & 0x1);
+  unsigned long r5 = ((phy >> 10) & 0x1);
+  unsigned long r6 = ((phy >> 11) & 0x1);
+  unsigned long r7 = ((phy >> 12) & 0x1);
+
+  // unsigned long bg0_old = r3 ^ ((addr>>6) & 0x1); //MAP21 ^ MAP6
+  unsigned long bg1_old = r5 ^ ((phy >> 6) & 0x1);  // MAP22 ^ MAP18
+  unsigned long b0_old = r6 ^ ((phy >> 7) & 0x1);
+  unsigned long b1_old = r7 ^ ((phy >> 8) & 0x1);
+
+  unsigned long row_old =
+      ((phy >> 3) & 0b111) + ((phy >> 6) & 0b1000) + ((phy >> 13) & 0b10000) +
+      ((phy >> 5) & 0b111111100000) + ((phy >> 6) & 0b111111000000000000);
+  unsigned long row_new = row_old + inc;
+
+  unsigned long phy_new =
+      ((row_new & 0xff000) << 6) +  //~R12 = ~A30 , ~PA18
+      ((row_new & 0x10) << 13) +    //~R4 = A29 , PA17
+      ((row_new & 0xfe0) << 5) +    // R11~R5 = A28~A22 , PA16~10
+      ((row_new & 0b1000) << 6) +   // R3 = A21, PA9
+      (((row_new & 0b10000000) ^ (b1_old << 7)) << 1) +
+      (((row_new & 0b1000000) ^ (b0_old << 6)) << 1) +
+      (((row_new & 0b100000) ^ (bg1_old << 5)) << 1) +
+      ((row_new & 0b111) << 3) +  // R2~R0 = A17~A15, PA5~PA3
+      ((rank & 0b1) << 1);        // Rank = A13, PA1
+  return (unsigned long)phy_new;
 }
 
-/* returns a pfn of a page "dec" rows below the page "phy" in a base row */ 
+/* returns a pfn of a page "dec" rows below the page "phy" in a base row */
 /*@input: phy - physical page in base DRAM row
 @input: dec - offset to the base row
 
 @return: pfn of page in the row base row - dec
 */
 
-static unsigned long get_row_minus(unsigned long phy, int dec){
-	unsigned long bank_old = get_bank(phy);
-	unsigned long row_new = (phy>>6) - dec;
-	unsigned long bank_new = (row_new & 0x7) ^ bank_old;
-	unsigned long rank_new = (phy>>7)&1;
-
-return  (unsigned long)((row_new << 6) | (rank_new << 5) | (bank_new << 2) | (phy & 0x3));
-
+static unsigned long get_row_minus(unsigned long phy, int dec) {
+  unsigned long rank = (phy >> 1) & 0x1;
+
+  // unsigned long r3 = ((phy >> 9) & 0x1);
+  unsigned long r5 = ((phy >> 10) & 0x1);
+  unsigned long r6 = ((phy >> 11) & 0x1);
+  unsigned long r7 = ((phy >> 12) & 0x1);
+
+  // unsigned long bg0_old = r3 ^ ((addr>>6) & 0x1); //MAP21 ^ MAP6
+  unsigned long bg1_old = r5 ^ ((phy >> 6) & 0x1);  // MAP22 ^ MAP18
+  unsigned long b0_old = r6 ^ ((phy >> 7) & 0x1);
+  unsigned long b1_old = r7 ^ ((phy >> 8) & 0x1);
+
+  unsigned long row_old =
+      ((phy >> 3) & 0b111) + ((phy >> 6) & 0b1000) + ((phy >> 13) & 0b10000) +
+      ((phy >> 5) & 0b111111100000) + ((phy >> 6) & 0b111111000000000000);
+  unsigned long row_new = row_old - dec;
+
+  unsigned long phy_new =
+      ((row_new & 0xff000) << 6) +  //~R12 = ~A30 , ~PA18
+      ((row_new & 0x10) << 13) +    //~R4 = A29 , PA17
+      ((row_new & 0xfe0) << 5) +    // R11~R5 = A28~A22 , PA16~10
+      ((row_new & 0b1000) << 6) +   // R3 = A21, PA9
+      (((row_new & 0b10000000) ^ (b1_old << 7)) << 1) +
+      (((row_new & 0b1000000) ^ (b0_old << 6)) << 1) +
+      (((row_new & 0b100000) ^ (bg1_old << 5)) << 1) +
+      ((row_new & 0b111) << 3) +  // R2~R0 = A17~A15, PA5~PA3
+      ((rank & 0b1) << 1);        // Rank = A13, PA1
+  return (unsigned long)phy_new;
 }
 
-
 /* convert virtual address from user process into physical address */
 /* @input: mm - memory discriptor user process
    @input: virt - virtual address
 
    @return: corresponding physical address of "virt" */
 
-static unsigned long virt_to_phy( struct mm_struct *mm,unsigned long virt)
-{
-	unsigned long phys;
-	struct page *pg;
-	int ret = get_user_pages ( NULL,
- 								mm,
- 								virt,
- 								1,
- 								0,
- 								0,
- 								&pg,
- 								NULL);
-
-	if(ret <= 0)
-		return 0;
-	/* get physical address */
-	phys = page_to_phys(pg);
-	return phys;
+static unsigned long virt_to_phy(struct mm_struct *mm, unsigned long virt) {
+  unsigned long phys;
+  struct page *pg;
+  //int ret = get_user_pages_remote(mm, virt, 1, 0, &pg, NULL, NULL);
+  int ret = get_user_pages_remote(NULL, mm, virt, 1, 0, &pg, NULL, NULL);
+  // printk("virt = %lx\n", virt);
+  if (ret <= 0) return 0;
+  /* get physical address */
+  phys = page_to_pfn(pg);
+  // printk("phys = %lx\n", phys);
+  return phys;
 }
 
 /* Interrupt handler for store sample */
 void precise_str_callback(struct perf_event *event,
-            				struct perf_sample_data *data,
-            				struct pt_regs *regs)
-{
-	/* Check source of store, if local dram (|0x80) record sample */
-	if(data->data_src.val & (1<<7)){
-	
-		sample_buffer[sample_head].phy_page = virt_to_phy(current->mm,data->addr)>>12;
-		if(sample_buffer[sample_head].phy_page > 0){
-			sample_buffer[sample_head].addr = data->addr;
-			/* limit sample index */
-			if(++sample_head > SAMPLES_MAX-1)
-				sample_head = SAMPLES_MAX-1;
-
-			sample_total++;
-		}
-	}
+                          struct perf_sample_data *data, struct pt_regs *regs) {
+  /* Check source of store, if local dram (|0x80) record sample */
+  if (data->data_src.val & (1 << 7)) {
+    sample_buffer[sample_head].phy_page =
+        virt_to_phy(current->mm, data->addr);  //>> 12;
+    if (sample_buffer[sample_head].phy_page > 0) {
+      sample_buffer[sample_head].addr = data->addr;
+      if (++sample_head > SAMPLES_MAX - 1) sample_head = SAMPLES_MAX - 1;
+      sample_total++;
+    }
+  }
 }
 
 /* Interrupt handler for load sample */
 void load_latency_callback(struct perf_event *event,
-            struct perf_sample_data *data,
-            struct pt_regs *regs)
-{	
-	sample_buffer[sample_head].phy_page = virt_to_phy(current->mm,data->addr)>>12;
-
-#ifdef DEBUG
-	sample_buffer[sample_head].addr = data->addr;
-	sample_buffer[sample_head].lat = data->weight;
-#endif
-
-	/* limit sample index */
-	if(++sample_head > SAMPLES_MAX-1)
-		sample_head = SAMPLES_MAX-1;
-	
-	sample_total++;
+                           struct perf_sample_data *data,
+                           struct pt_regs *regs) {
+  sample_buffer[sample_head].phy_page =
+      virt_to_phy(current->mm, data->addr);  //>> 12;
+  if (sample_buffer[sample_head].phy_page > 0) {
+    if (++sample_head > SAMPLES_MAX - 1) sample_head = SAMPLES_MAX - 1;
+    sample_total++;
+  }
 }
 
-void llc_event_wq_callback(struct work_struct *work)
-{
-	int cpu;
-	u64 enabled,running;
-	u64 ld_miss;
-
-	/* If we were sampling, stop sampling and analyze samples */
-	if(sampling){
-		/* stop sampling */
-		for_each_online_cpu(cpu){
-			perf_event_disable(per_cpu(ld_lat_event,cpu));
-			perf_event_disable(per_cpu(precise_str_event,cpu));
-		}
-		sampling = 0;			
-		/* start task that anayzes samples and take action */ 
-		queue_work(action_wq, &task);
-	}							
-	else if(start_sampling){
-		/* update MEM_LOAD_UOPS_MISC_RETIRED_LLC_MISS value */
-		l1D_val = 0;
-		for_each_online_cpu(cpu){
-        	l1D_val += perf_event_read_value(per_cpu(l1D_event,cpu), 
-											&enabled, &running);
-		}
-							
-		ld_miss = l1D_val - old_l1D_val;
-						
-		/* Sample loads, stores or both based on LLC load miss count */
-		if(ld_miss >= (miss_total*9)/10){
-			for_each_online_cpu(cpu){
-				perf_event_enable(per_cpu(ld_lat_event,cpu));//sample loads only
-			}
-		}
-
-		else if(ld_miss < miss_total/10){
-			for_each_online_cpu(cpu){
-				perf_event_enable(per_cpu(precise_str_event,cpu));//sample stores only
-			}
-		}
-
-		else{
-			for_each_online_cpu(cpu){
-				/* sample both */
-				perf_event_enable(per_cpu(ld_lat_event,cpu));
-				perf_event_enable(per_cpu(precise_str_event,cpu));
-			}			
-		}
-							
-		sample_total = 0;
-		record_size = 0;
-		sample_head = 0;
-			
-		/* log how many times we passed the threshold */
-		L1_count++;
-		start_sampling = 0;
-		sampling = 1;
-	}
-
-	old_l1D_val = l1D_val;
+void llc_event_wq_callback(struct work_struct *work) {
+  int cpu;
+  u64 enabled, running;
+  u64 ld_miss, st_miss;
+  // printk(">>> llc_event_wq_callback <<<\n");
+  /* If we were sampling, stop sampling and analyze samples */
+  // printk(">>> sampling : %d <<<\n", sampling);
+  // printk(">>> start_sampling : %d <<<\n", start_sampling);
+  if (sampling) {
+    /* stop sampling */
+    mutex_lock(&disable_sampling);
+    for_each_online_cpu(cpu) {
+      perf_event_disable(per_cpu(ld_lat_event, cpu));
+      perf_event_disable(per_cpu(precise_str_event, cpu));
+    }
+    mutex_unlock(&disable_sampling);
+    sampling = 0;
+    /* start task that anayzes samples and take action */
+    queue_work(action_wq, &task);
+  } else if (start_sampling) {
+    /* update MEM_LOAD_UOPS_MISC_RETIRED_LLC_MISS value */
+    ld_val = 0;
+    st_val = 0;
+    mutex_lock(&count_sampling);
+    for_each_online_cpu(cpu) {
+      ld_val +=
+          perf_event_read_value(per_cpu(ld_event, cpu), &enabled, &running);
+      st_val +=
+          perf_event_read_value(per_cpu(st_event, cpu), &enabled, &running);
+    }
+    mutex_unlock(&count_sampling);
+    ld_miss = ld_val - old_ld_val;
+    st_miss = st_val - old_st_val;
+
+    /* Sample loads, stores or both based on LLC load miss count */
+    if ((ld_miss / 9) > st_miss) {
+      mutex_lock(&enable_sampling);
+      for_each_online_cpu(cpu) {
+        perf_event_enable(per_cpu(ld_lat_event, cpu));  // sample loads only
+      }
+      mutex_unlock(&enable_sampling);
+    } else if ((st_miss / 9) > ld_miss) {
+      mutex_lock(&enable_sampling);
+      for_each_online_cpu(cpu) {
+        perf_event_enable(
+            per_cpu(precise_str_event, cpu));  // sample stores only
+      }
+      mutex_unlock(&enable_sampling);
+    } else {
+      mutex_lock(&enable_sampling);
+      for_each_online_cpu(cpu) { /* sample both */
+        perf_event_enable(per_cpu(ld_lat_event, cpu));
+        perf_event_enable(per_cpu(precise_str_event, cpu));
+      }
+      mutex_unlock(&enable_sampling);
+    }
+
+    sample_total = 0;
+    record_size = 0;
+    sample_head = 0;
+    start_sampling = 0;
+    sampling = 1;
+  }
+  old_ld_val = ld_val;
+  old_st_val = st_val;
 }
 
 /* look at sample profile and take action */
-void action_wq_callback( struct work_struct *work)
-{
-	int rec,log_;
-	unsigned long pfn1,pfn2;
-	unsigned long *virt;
-	struct page *pg1,*pg2;
-	int i;
-		
-	/* group samples based on physical pages */
-	build_profile();
-	/* sort profile, address with highest number
-	of samples first */
-	sort();
-
-#ifdef DEBUG
-	log_=0;
-#endif
-
-	if(miss_total > LLC_MISS_THRESHOLD){//if still  high miss
-		printk("samples = %u\n",sample_total);
-		/* calculate hammer threshold */
-		hammer_threshold = (LLC_MISS_THRESHOLD*sample_total)/miss_total;
+void action_wq_callback(struct work_struct *work) {
+  int rec;
+  // int rec, log_;
+  unsigned long pfn1, pfn2;
+  unsigned long *virt;
+  struct page *pg1, *pg2;
+  int i;
+
+  /* group samples based on physical pages */
+  build_profile();
+  /* sort profile, address with highest number
+  of samples first */
+  sort();
 
-		/* check for potential agressors */
-		for(rec = 0;rec<record_size;rec++){
-#ifdef DEBUG
-			profile[rec].hammer = 0;
-#endif
-			if((profile[rec].llc_total_miss >= hammer_threshold/2) && (sample_total>= MIN_SAMPLES)){
-#ifdef DEBUG
-				log_ = 1;
-				profile[rec].hammer = 1;
-				L2_count++;
-#endif
-				/* potential hammering detected , deploy refresh */
-				for(i=1;i<=REFRESHED_ROWS;i++){
-				/* get page frame number for pages in rows above and below */
-					pfn1 = get_row_plus(profile[rec].phy_page,i);// pfn for victim row1
-					pfn2 = get_row_minus(profile[rec].phy_page,i);// pfn for victim row2
-
-					/* get physical page */
-					pg1 = pfn_to_page(pfn1);
-					pg2 = pfn_to_page(pfn2);
-			
-					/* map pages to kernel space and refresh */
-					virt = (unsigned long*)kmap(pg1);
-					if(virt){
-						asm volatile("clflush (%0)"::"r"(virt):"memory");
-						get_user(profile[rec].dummy1,virt);
-						kunmap(pg1);
-					}
-
-					virt = (unsigned long*)kmap(pg2);
-					if(virt){
-						asm volatile("clflush (%0)"::"r"(virt):"memory");
-						get_user(profile[rec].dummy2,virt);
-						kunmap(pg2);
-					}
-				}
 #ifdef DEBUG
-				refresh_count++;
+  log_ = 0;
 #endif
-			}
-		}
-	}
-
+  // printk(">>> enter action wq <<<\n");
+  // printk(">>> miss total : %lld <<<\n", miss_total);
+  if (miss_total > LLC_MISS_THRESHOLD) {  // if still  high miss
+                                          // if (0) {  // by pass for test
+    /* calculate hammer threshold */
+    // hammer_threshold = (LLC_MISS_THRESHOLD * sample_total) / miss_total;
+    hammer_threshold = 4;
+    // printk(">>> hammer_threshold : %d <<<\n", hammer_threshold);
+    // printk(">>> LLC_MISS_THRESHOLD : %d <<<\n", LLC_MISS_THRESHOLD);
+    // printk(">>> sample_total : %d <<<\n", sample_total);
+    // printk(">>> miss_total : %lld <<<\n", miss_total);
+    //  printk(">>> enter action wq <<<\n");
+    //  printk(">>> hammer threshold : %d <<<\n", hammer_threshold);
+    /* check for potential agressors */
+    // mutex_lock(&get_user_mutex);
+    for (rec = 0; rec < record_size; rec++) {
+      // #ifdef DEBUG
+      //       profile[rec].hammer = 0;
+      // #endif
+      // printk(">>> profile[rec].llc_total_miss : %ld <<<\n",
+      // profile[rec].llc_total_miss); printk(">>> hammer_threshold/2 : %d
+      // <<<\n", hammer_threshold / 2);
+      if ((profile[rec].llc_total_miss >= hammer_threshold / 2) &&
+          (sample_total >= MIN_SAMPLES)) {
+        // #ifdef DEBUG
+        //         log_ = 1;
+        //         profile[rec].hammer = 1;
+        //         L2_count++;
+        // #endif
+        // printk(">>> REFRESHED_ROWS : %d <<<\n", REFRESHED_ROWS);
+        /* potential hammering detected , deploy refresh */
+        // printk(">>> pfn : %lx <<<\n", profile[rec].phy_page);
+        if (profile[rec].phy_page > 0x400000) {
+          for (i = 1; i <= REFRESHED_ROWS; i++) {
+            /* get page frame number for pages in rows above and below */
+            pfn1 =
+                get_row_plus(profile[rec].phy_page, i);  // pfn for victim row1
+            pfn2 =
+                get_row_minus(profile[rec].phy_page, i);  // pfn for victim row2
+            // printk(">>> pfn : %lx <<<\n", profile[rec].phy_page);
+            // printk(">>> pfn1 : %lx <<<\n", pfn1);
+            // printk(">>> pfn2 : %lx <<<\n", pfn2);
+            /* get physical page */
+            if (pfn_valid(pfn1) > 0) {
+              pg1 = pfn_to_page(pfn1);
+              virt = (unsigned long *)kmap(pg1);
+
+              if (virt) {
+                if (virt_addr_valid(virt) > 0) {
+                  asm volatile("clflush (%0)" ::"r"(virt) : "memory");
+                  mutex_lock(&get_user_mutex1);
+                  // down(&get_user_sem);
+                  get_user(profile[rec].dummy1, virt);
+                  mutex_unlock(&get_user_mutex1);
+                  // up(&get_user_sem);
+                  // printk(">>> virt addr1 valid <<<\n");
+                } else {
+                  printk(">>> virt addr2 unvalid <<<\n");
+                }
+                kunmap(pg1);
+              }
+            } else {
+              printk(">>> pfn1 unvalid <<<\n");
+            }
+            if (pfn_valid(pfn2) > 0) {
+              pg2 = pfn_to_page(pfn2);
+              virt = (unsigned long *)kmap(pg2);
+
+              if (virt) {
+                if (virt_addr_valid(virt) > 0) {
+                  asm volatile("clflush (%0)" ::"r"(virt) : "memory");
+                  mutex_lock(&get_user_mutex2);
+                  // down(&get_user_sem);
+                  get_user(profile[rec].dummy2, virt);
+                  mutex_unlock(&get_user_mutex2);
+                  // up(&get_user_sem);
+                  // printk(">>> virt addr2 valid <<<\n");
+                } else {
+                  printk(">>> virt addr2 unvalid <<<\n");
+                }
+                kunmap(pg2);
+              }
+            } else {
+              printk(">>> pfn2 unvalid <<<\n");
+            }
+          }
+        } else {
+          // printk(">>> non-refreshed pfn : %lx <<<\n", profile[rec].phy_page);
+        }
 #ifdef DEBUG
-	if(log_){
-		for(rec = 0;rec<record_size;rec++){
-			log[log_index].profile[rec].phy_page = profile[rec].phy_page;
-			log[log_index].profile[rec].llc_percent_miss = profile[rec].llc_percent_miss;
-			log[log_index].profile[rec].dummy1 = profile[rec].dummy1;
-			log[log_index].profile[rec].dummy2 = profile[rec].dummy2;
-			log[log_index].profile[rec].hammer = profile[rec].hammer;
-		}
-		log[log_index].record_size = record_size;
-		log[log_index].sample_total = sample_total;
-		log_index++;
-		if(log_index > 24999)
-			log_index = 24999;
-	}
+        refresh_count++;
 #endif
-	return;
+      }
+    }
+    // mutex_unlock(&get_user_mutex);
+  }
+
+  // #ifdef DEBUG
+  //   if (log_) {
+  //     for (rec = 0; rec < record_size; rec++) {
+  //       log[log_index].profile[rec].phy_page = profile[rec].phy_page;
+  //       log[log_index].profile[rec].llc_percent_miss =
+  //           profile[rec].llc_percent_miss;
+  //       log[log_index].profile[rec].dummy1 = profile[rec].dummy1;
+  //       log[log_index].profile[rec].dummy2 = profile[rec].dummy2;
+  //       log[log_index].profile[rec].hammer = profile[rec].hammer;
+  //     }
+  //     log[log_index].record_size = record_size;
+  //     log[log_index].sample_total = sample_total;
+  //     log_index++;
+  //     if (log_index > 24999) log_index = 24999;
+  //   }
+  // #endif
+  return;
 }
 
 /* Timer interrupt handler */
-enum hrtimer_restart timer_callback( struct hrtimer *timer )
-{
-	ktime_t now;
-	u64 enabled,running;
-	int cpu;
-        
-    /* Update llc miss counter value */
-	val = 0;
-	for_each_online_cpu(cpu){
-    	val += perf_event_read_value(per_cpu(llc_event,cpu), &enabled, &running);
-	}
-
-	miss_total = val - old_val;
-	old_val = val;
-	if(!sampling){
-	/* Start sampling if miss rate is high */
-		if(miss_total > LLC_MISS_THRESHOLD){
-			start_sampling = 1;
-			/* set next interrupt interval for sampling */
-			ktime = ktime_set(0,sample_timer_period);
-      		now = hrtimer_cb_get_time(timer); 
-      		hrtimer_forward(&sample_timer,now,ktime);
-		}
-
-		else{
-			/* set next interrupt interval for counting */
-			ktime = ktime_set(0,count_timer_period);
-     		now = hrtimer_cb_get_time(timer); 
-      		hrtimer_forward(&sample_timer,now,ktime);
-		}
-	}
-
-	else{
-		ktime = ktime_set(0,count_timer_period);
-     	now = hrtimer_cb_get_time(timer); 
-      	hrtimer_forward(&sample_timer,now,ktime);
-	}
-				
-	/* start task that analyzes llc misses */
-	queue_work(llc_event_wq, &task2);
-
-	/* restart timer */
-   	return HRTIMER_RESTART;
+enum hrtimer_restart timer_callback(struct hrtimer *timer) {
+  ktime_t now;
+  u64 enabled, running;
+  int cpu;
+
+  /* Update llc miss counter value */
+  val = 0;
+  // mutex_lock(&llc_check);
+  for_each_online_cpu(cpu) {
+    val += perf_event_read_value(per_cpu(llc_event, cpu), &enabled, &running);
+  }
+  // mutex_lock(&llc_check);
+  miss_total = val - old_val;
+  old_val = val;
+  if (!sampling) {
+    /* Start sampling if miss rate is high */
+    // printk(">>> miss_total : %lld <<<\n", miss_total);
+    if (miss_total > LLC_MISS_THRESHOLD) {
+      start_sampling = 1;
+      /* set next interrupt interval for sampling */
+      ktime = ktime_set(0, sample_timer_period);
+      now = hrtimer_cb_get_time(timer);
+      hrtimer_forward(&sample_timer, now, ktime);
+    }
+
+    else {
+      /* set next interrupt interval for counting */
+      ktime = ktime_set(0, count_timer_period);
+      now = hrtimer_cb_get_time(timer);
+      hrtimer_forward(&sample_timer, now, ktime);
+    }
+  }
+
+  else {
+    ktime = ktime_set(0, count_timer_period);
+    now = hrtimer_cb_get_time(timer);
+    hrtimer_forward(&sample_timer, now, ktime);
+  }
+
+  /* start task that analyzes llc misses */
+  queue_work(llc_event_wq, &task2);
+
+  /* restart timer */
+  return HRTIMER_RESTART;
 }
 
 /* Groups samples accoriding to accessed physical pages */
-static void build_profile(void)
-{
-	int rec,smpl,recorded;
-	sample_t sample;
-
-	if(sample_total > 0){
-		sample = sample_buffer[0];
-		profile[0].phy_page 	= sample.phy_page;
-		profile[0].page = (sample.addr);
-		profile[0].llc_total_miss = 1;
-		profile[0].llc_percent_miss = 100;
-		profile[0].cpu 	= sample.cpu;
-		record_size = 1;
-			
-		for(smpl=1; smpl<sample_head; smpl++){
-			sample = sample_buffer[smpl];
-
-			/* see if page already exists */
-			recorded = 0;
-			for(rec=0;rec<record_size;rec++){
-				if((profile[rec].phy_page != 0) && 
-					(profile[rec].phy_page == sample.phy_page)){
-					profile[rec].llc_total_miss++;
-					profile[rec].cpu 	= sample.cpu;
-					recorded = 1;
-					break;
-				}
-			}
-
-			if(!recorded){
-				/* must be new record */
-				/* If there is space in the profile add new record
-				else replace the last one  (The least miss in the profile) */
-				if(record_size < PROFILE_N){
-					profile[record_size].phy_page 	= sample.phy_page;
-					profile[record_size].page = (sample.addr);
-					profile[record_size].llc_total_miss = 1;
-					profile[record_size].cpu = sample.cpu;
-					record_size++;
-				}
-
-				else{
-					profile[record_size - 1].phy_page = sample.phy_page;
-					profile[record_size - 1].page = (sample.addr);
-					profile[record_size - 1].llc_total_miss = 1;
-					profile[record_size - 1].cpu = sample.cpu;
-				}
-			}
-		}
+static void build_profile(void) {
+  int rec, smpl, recorded;
+  sample_t sample;
+  // printk("sample_total = %d\n", sample_total);
+  if (sample_total > 0) {
+    sample = sample_buffer[0];
+    profile[0].phy_page = sample.phy_page;
+    profile[0].page = (sample.addr);
+    profile[0].llc_total_miss = 1;
+    profile[0].llc_percent_miss = 100;
+    profile[0].cpu = sample.cpu;
+    record_size = 1;
+
+    for (smpl = 1; smpl < sample_head; smpl++) {
+      sample = sample_buffer[smpl];
+
+      /* see if page already exists */
+      recorded = 0;
+      for (rec = 0; rec < record_size; rec++) {
+        if ((profile[rec].phy_page != 0) &&
+            (profile[rec].phy_page == sample.phy_page)) {
+          profile[rec].llc_total_miss++;
+          profile[rec].cpu = sample.cpu;
+          recorded = 1;
+          break;
+        }
+      }
+
+      if (!recorded) {
+        /* must be new record */
+        /* If there is space in the profile add new record
+        else replace the last one  (The least miss in the profile) */
+        if (record_size < PROFILE_N) {
+          profile[record_size].phy_page = sample.phy_page;
+          profile[record_size].page = (sample.addr);
+          profile[record_size].llc_total_miss = 1;
+          profile[record_size].cpu = sample.cpu;
+          record_size++;
+        }
+
+        else {
+          profile[record_size - 1].phy_page = sample.phy_page;
+          profile[record_size - 1].page = (sample.addr);
+          profile[record_size - 1].llc_total_miss = 1;
+          profile[record_size - 1].cpu = sample.cpu;
+        }
+      }
+    }
 
 #ifdef DEBUG
-		/* calculate percentage */
-		for(rec=0;rec<record_size;rec++){
-			profile[rec].llc_percent_miss = (profile[rec].llc_total_miss*100)/sample_total;
-		}
+    /* calculate percentage */
+    for (rec = 0; rec < record_size; rec++) {
+      profile[rec].llc_percent_miss =
+          (profile[rec].llc_total_miss * 100) / sample_total;
+    }
 #endif
-	}
+  }
 }
 
 /* Sort addresses with higest address distribution first */
-static void sort(void){
-	int swapped,rec;
-	do{
-		swapped = 0;
-		for(rec = 1; rec<record_size; rec++){
-			if(profile[rec-1].llc_percent_miss < profile[rec].llc_percent_miss){
-				profile_t temp = profile[rec-1];
-				profile[rec-1] = profile[rec];
-				profile[rec] = temp;
-				swapped = 1;
-			}
-		}
-	}while(swapped);
+static void sort(void) {
+  int swapped, rec;
+  do {
+    swapped = 0;
+    for (rec = 1; rec < record_size; rec++) {
+      if (profile[rec - 1].llc_percent_miss < profile[rec].llc_percent_miss) {
+        profile_t temp = profile[rec - 1];
+        profile[rec - 1] = profile[rec];
+        profile[rec] = temp;
+        swapped = 1;
+      }
+    }
+  } while (swapped);
 }
 
-
 /* Initialize module */
-static int start_init(void)
-{
-	int cpu;
-	old_val = 0;
-	/* Setup LLC Miss event */
-	for_each_online_cpu(cpu){
-   		per_cpu(llc_event, cpu) = perf_event_create_kernel_counter(&llc_miss_event, cpu,
-                 NULL,llc_event_callback,NULL);
-   	 	if(IS_ERR(per_cpu(llc_event, cpu))){
-        	printk("Error creating llc event.\n");
-        	return 0;
-    	}				
-		/* start counting */
-		perf_event_enable(per_cpu(llc_event, cpu));
-	}
-
-	old_l1D_val = 0;
-	/* setup LLC Miss event */
-	for_each_online_cpu(cpu){
-   		per_cpu(l1D_event, cpu) = perf_event_create_kernel_counter(&l1D_miss_event, cpu,
-                 NULL,l1D_event_callback,NULL);
-   	 	if(IS_ERR(per_cpu(l1D_event, cpu))){
-        	printk("Error creating l1D miss event.\n");
-        	return 0;
-    	}
-						
-		/* start counting */
-		perf_event_enable(per_cpu(l1D_event, cpu));
-	}
-
-	/* setup load latency event */
-	for_each_online_cpu(cpu){
-   		per_cpu(ld_lat_event, cpu) = perf_event_create_kernel_counter(&load_latency_event, cpu,
-                 													NULL,load_latency_callback,NULL);
-   	 	if(IS_ERR(per_cpu(ld_lat_event, cpu))){
-        	printk("Error creating load latency event.\n");
-        	return 0;
-    	}
-	}
-
-	/* setup precise store event */
-	for_each_online_cpu(cpu){
-   		per_cpu(precise_str_event, cpu) = perf_event_create_kernel_counter(&precise_str_event_attr, cpu,
-                 															NULL,precise_str_callback,NULL);
-   	 	if(IS_ERR(per_cpu(precise_str_event, cpu))){
-        	printk("Error creating precise store event.\n");
-        	return 0;
-    	}
-	}
-
-	/* setup Timer */
-    ktime = ktime_set(0,count_timer_period);
-    hrtimer_init(&sample_timer,CLOCK_REALTIME,HRTIMER_MODE_REL);
-    sample_timer.function = &timer_callback;
-    hrtimer_start(&sample_timer,ktime,HRTIMER_MODE_REL);
-     
-	/* initialize work queue */
-	action_wq = create_workqueue("action_queue");
-	INIT_WORK(&task, action_wq_callback);
-
-	llc_event_wq = create_workqueue("llc_event_queue");
-	INIT_WORK(&task2, llc_event_wq_callback);
-
-	printk("done initializing\n");
-  	
-   	return 0;
+static int start_init(void) {
+  int cpu;
+  old_val = 0;
+  /* Setup LLC Miss event */
+  for_each_online_cpu(cpu) {
+    per_cpu(llc_event, cpu) = perf_event_create_kernel_counter(
+        &llc_miss_event, cpu, NULL, llc_event_callback, NULL);
+    if (IS_ERR(per_cpu(llc_event, cpu))) {
+      printk("Error creating llc event.\n");
+      return 0;
+    }
+    /* start counting */
+    perf_event_enable(per_cpu(llc_event, cpu));
+  }
+
+  old_ld_val = 0;
+  /* setup LLC Miss event */
+  for_each_online_cpu(cpu) {
+    per_cpu(ld_event, cpu) = perf_event_create_kernel_counter(
+        &ld_miss_event, cpu, NULL, ld_event_callback, NULL);
+    if (IS_ERR(per_cpu(ld_event, cpu))) {
+      printk("Error creating ld miss event.\n");
+      return 0;
+    }
+
+    /* start counting */
+    perf_event_enable(per_cpu(ld_event, cpu));
+  }
+
+  old_st_val = 0;
+  /* setup LLC Miss event */
+  for_each_online_cpu(cpu) {
+    per_cpu(st_event, cpu) = perf_event_create_kernel_counter(
+        &st_miss_event, cpu, NULL, st_event_callback, NULL);
+    if (IS_ERR(per_cpu(st_event, cpu))) {
+      printk("Error creating st miss event.\n");
+      return 0;
+    }
+
+    /* start counting */
+    perf_event_enable(per_cpu(st_event, cpu));
+  }
+
+  /* setup load latency event */
+  for_each_online_cpu(cpu) {
+    per_cpu(ld_lat_event, cpu) = perf_event_create_kernel_counter(
+        &load_latency_event, cpu, NULL, load_latency_callback, NULL);
+    if (IS_ERR(per_cpu(ld_lat_event, cpu))) {
+      printk("Error creating load latency event.\n");
+      return 0;
+    }
+  }
+
+  /* setup precise store event */
+  for_each_online_cpu(cpu) {
+    per_cpu(precise_str_event, cpu) = perf_event_create_kernel_counter(
+        &precise_str_event_attr, cpu, NULL, precise_str_callback, NULL);
+    if (IS_ERR(per_cpu(precise_str_event, cpu))) {
+      printk("Error creating precise store event.\n");
+      return 0;
+    }
+  }
+
+  /* setup Timer */
+  ktime = ktime_set(0, count_timer_period);
+  hrtimer_init(&sample_timer, CLOCK_REALTIME, HRTIMER_MODE_REL);
+  sample_timer.function = &timer_callback;
+  hrtimer_start(&sample_timer, ktime, HRTIMER_MODE_REL);
+
+  /* initialize work queue */
+  action_wq = create_workqueue("action_queue");
+  INIT_WORK(&task, action_wq_callback);
+
+  llc_event_wq = create_workqueue("llc_event_queue");
+  INIT_WORK(&task2, llc_event_wq_callback);
+
+  printk("done initializing\n");
+
+  return 0;
 }
 
 /* Cleanup module */
-static void finish_exit(void)
-{
-    int ret,cpu,i,j; 
-    /* timer */
-    ret = hrtimer_cancel(&sample_timer);
-
-    /* llc_event */
-	for_each_online_cpu(cpu){
-    	if(per_cpu(llc_event, cpu)){
-    		perf_event_disable(per_cpu(llc_event, cpu));
-       		perf_event_release_kernel(per_cpu(llc_event, cpu));
-    	}
-	}
-
-	/* l1D_event */
-	for_each_online_cpu(cpu){
-    	if(per_cpu(l1D_event, cpu)){
-    		perf_event_disable(per_cpu(l1D_event, cpu));
-        	perf_event_release_kernel(per_cpu(l1D_event, cpu));
-    	}
-	}
-
-	/* load latency event */
-	for_each_online_cpu(cpu){
-    	if(per_cpu(ld_lat_event, cpu)){
-        	perf_event_disable(per_cpu(ld_lat_event, cpu));
-        	perf_event_release_kernel(per_cpu(ld_lat_event, cpu));
-    	}
-	}
-
-	/* precise store event */
-	for_each_online_cpu(cpu){
-    	if(per_cpu(precise_str_event, cpu)){
-    		perf_event_disable(per_cpu(precise_str_event, cpu));
-        	perf_event_release_kernel(per_cpu(precise_str_event, cpu));
-   		 }
-	}
-
-	flush_workqueue(action_wq);
-  	destroy_workqueue(action_wq);
-	flush_workqueue(llc_event_wq);
-  	destroy_workqueue(llc_event_wq);
-
-#ifdef DEBUG
-	/* Log of ANVIL. CSV of some of the sampled/detected addresses */
-			 
-	printk(">>>>>>>>>>>>>>>>log dump>>>>>>>>>>>>>>>\n");
-	/* dump all the logs */
-	for(i=0; i<log_index; i++)
-	{
-		for(j=0; j<4; j++){
-			/* physical pages */
-			printk("%lu,",log[i].profile[j].phy_page);
-		}
-
-		for(j=0; j<4; j++){
-			/* Values read form row above and row below */
-			printk("%d,",log[i].profile[j].hammer);
-			printk("0x%lx,",log[i].profile[j].dummy1);
-			printk("0x%lx,",log[i].profile[j].dummy2);
-		}
-					
-		/* Total samples per sample period */
-		printk("%u\n",log[i].sample_total);
-	}
-	/* L1 count: Number of times LLC_MISS_THRESHOLD was crossed
-	   L2 count: Number of times potential hammer activity was detected
-	   Refresh: Number of addresses that resulted in refreshes */
-	printk("L1 count = %lu\n",L1_count);
-	printk("L2 count = %lu\n",L2_count);
-	printk("Refreshs = %lu\n",refresh_count);
-	printk(">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n");
-    return;
-#endif
+static void finish_exit(void) {
+  // int ret, cpu, i, j;
+  int ret, cpu;
+  /* timer */
+  ret = hrtimer_cancel(&sample_timer);
+
+  /* llc_event */
+  for_each_online_cpu(cpu) {
+    if (per_cpu(llc_event, cpu)) {
+      perf_event_disable(per_cpu(llc_event, cpu));
+      perf_event_release_kernel(per_cpu(llc_event, cpu));
+    }
+  }
+
+  /* ld_miss_event */
+  for_each_online_cpu(cpu) {
+    if (per_cpu(ld_event, cpu)) {
+      perf_event_disable(per_cpu(ld_event, cpu));
+      perf_event_release_kernel(per_cpu(ld_event, cpu));
+    }
+  }
+
+  /* st_miss_event */
+  for_each_online_cpu(cpu) {
+    if (per_cpu(st_event, cpu)) {
+      perf_event_disable(per_cpu(st_event, cpu));
+      perf_event_release_kernel(per_cpu(st_event, cpu));
+    }
+  }
+
+  /* load latency event */
+  for_each_online_cpu(cpu) {
+    if (per_cpu(ld_lat_event, cpu)) {
+      perf_event_disable(per_cpu(ld_lat_event, cpu));
+      perf_event_release_kernel(per_cpu(ld_lat_event, cpu));
+    }
+  }
+
+  /* precise store event */
+  for_each_online_cpu(cpu) {
+    if (per_cpu(precise_str_event, cpu)) {
+      perf_event_disable(per_cpu(precise_str_event, cpu));
+      perf_event_release_kernel(per_cpu(precise_str_event, cpu));
+    }
+  }
+
+  flush_workqueue(action_wq);
+  destroy_workqueue(action_wq);
+  flush_workqueue(llc_event_wq);
+  destroy_workqueue(llc_event_wq);
 }
 
 module_init(start_init);
diff --git a/anvil/anvil.h b/anvil/anvil.h
index b6b166a..de1acb4 100644
--- a/anvil/anvil.h
+++ b/anvil/anvil.h
@@ -1,26 +1,25 @@
 
 #include <linux/perf_event.h>
 
-
 #define LOAD_LATENCY_EVENT 0x01CD
 #define PRECISE_STORE_EVENT 0x02CD
 #define MEM_LOAD_UOPS_MISC_RETIRED_LLC_MISS 0x02D4
 
 /* controls load sampling rate */
-#define LD_LAT_SAMPLE_PERIOD 50		
+#define LD_LAT_SAMPLE_PERIOD 50
 
 /* controls store sampling rate */
 #define PRE_STR_SAMPLE_PERIOD 3000
 
 /* count period in nanoseconds */
-#define count_timer_period 6000000 	
+#define count_timer_period 6000000
 
 /* sample period  in nanoseconds */
-#define sample_timer_period 6000000 					
+#define sample_timer_period 6000000
 
 /* last-level cache miss rate threshold
-			that triggers sampling            */
-#define LLC_MISS_THRESHOLD			20000
+                        that triggers sampling            */
+#define LLC_MISS_THRESHOLD 70000
 
 /* Maximum number of addresses in the address profile */
 #define PROFILE_N 20
@@ -31,88 +30,103 @@
 /* LLC miss event attribute */
 static struct perf_event_attr llc_miss_event = {
     .type = PERF_TYPE_HARDWARE,
-    .config = PERF_COUNT_HW_CACHE_MISSES,    
-    .exclude_user  	= 0,      
-    .exclude_kernel = 1,        
-				.pinned = 1,
+    .config = PERF_COUNT_HW_CACHE_MISSES,
+    .exclude_user = 0,
+    .exclude_kernel = 1,
+    .pinned = 1,
 };
 
 /* Load uops that misses LLC */
-static struct perf_event_attr l1D_miss_event = {
-    .type =  PERF_TYPE_RAW,
-    .config = MEM_LOAD_UOPS_MISC_RETIRED_LLC_MISS,    
-    .exclude_user  	= 0,       
-    .exclude_kernel = 1,        
-				.pinned = 1,
+// static struct perf_event_attr l1D_miss_event = {
+//     .type = PERF_TYPE_RAW,
+//     .config = MEM_LOAD_UOPS_MISC_RETIRED_LLC_MISS,
+//     .exclude_user = 0,
+//     .exclude_kernel = 1,
+//     .pinned = 1,
+// };
+
+/* Load uops that misses LLC */
+static struct perf_event_attr ld_miss_event = {
+    .type = PERF_TYPE_HW_CACHE,
+    .config = PERF_COUNT_HW_CACHE_LL | (PERF_COUNT_HW_CACHE_OP_READ << 8) |
+              (PERF_COUNT_HW_CACHE_RESULT_MISS << 16),
+    .exclude_user = 0,
+    .exclude_kernel = 1,
+    .pinned = 1,
 };
 
+/* Store uops that misses LLC */
+static struct perf_event_attr st_miss_event = {
+    .type = PERF_TYPE_HW_CACHE,
+    .config = PERF_COUNT_HW_CACHE_LL | (PERF_COUNT_HW_CACHE_OP_WRITE << 8) |
+              (PERF_COUNT_HW_CACHE_RESULT_MISS << 16),
+    .exclude_user = 0,
+    .exclude_kernel = 1,
+    .pinned = 1,
+};
 
 /* Load latency event attribute */
 static struct perf_event_attr load_latency_event = {
     .type = PERF_TYPE_RAW,
-    .config = LOAD_LATENCY_EVENT, 
-				.config1 = 150, //latency?   
-				.sample_type = 
-																			PERF_SAMPLE_ADDR 				|			//Sample address
-																			PERF_SAMPLE_DATA_SRC | 		//Sample data source 
-																			PERF_SAMPLE_WEIGHT,						//Sample latency in clock cycles
-				.sample_period = 	LD_LAT_SAMPLE_PERIOD, //How many samples before overflow(interrupt)
-    .exclude_user  = 0,        													//count user
-    .exclude_kernel = 1,        												//don't count kernel
-				.precise_ip 				= 1,																				// Enables precise event
-				.wakeup_events 	= 1,																				//overflow on each sample
-				.disabled = 1,
-				.pinned = 1,
+    .config = LOAD_LATENCY_EVENT,
+    .config1 = 150,                        // latency?
+    .sample_type = PERF_SAMPLE_ADDR |      // Sample address
+                   PERF_SAMPLE_DATA_SRC |  // Sample data source
+                   PERF_SAMPLE_WEIGHT,     // Sample latency in clock cycles
+    .sample_period =
+        LD_LAT_SAMPLE_PERIOD,  // How many samples before overflow(interrupt)
+    .exclude_user = 0,         // count user
+    .exclude_kernel = 1,       // don't count kernel
+    .precise_ip = 1,           // Enables precise event
+    .wakeup_events = 1,        // overflow on each sample
+    .disabled = 1,
+    .pinned = 1,
 };
 
 /*precise store event*/
 static struct perf_event_attr precise_str_event_attr = {
     .type = PERF_TYPE_RAW,
-    .config = PRECISE_STORE_EVENT,   
-				.sample_type =
-																			PERF_SAMPLE_ADDR 				|		
-																			PERF_SAMPLE_DATA_SRC , 	
-																		
-				.sample_period = 	PRE_STR_SAMPLE_PERIOD, 
-    .exclude_user   = 0,        						
-    .exclude_kernel = 1,        												
-				.precise_ip 				= 1,																				
-				.wakeup_events 	= 1,
-				.disabled 						= 1,
-				.pinned 								= 1,
+    .config = PRECISE_STORE_EVENT,
+    .sample_type = PERF_SAMPLE_ADDR | PERF_SAMPLE_DATA_SRC,
+
+    .sample_period = PRE_STR_SAMPLE_PERIOD,
+    .exclude_user = 0,
+    .exclude_kernel = 1,
+    .precise_ip = 1,
+    .wakeup_events = 1,
+    .disabled = 1,
+    .pinned = 1,
 };
 
 /* Address profile */
-typedef struct{
-	unsigned long phy_page;
-	unsigned long page;
-	int ld_st;
-	unsigned long llc_total_miss;
-	unsigned int llc_percent_miss;
-	int cpu;
-unsigned long dummy1;
-unsigned long dummy2;
-int hammer;
+typedef struct {
+  unsigned long phy_page;
+  unsigned long page;
+  int ld_st;
+  unsigned long llc_total_miss;
+  unsigned int llc_percent_miss;
+  int cpu;
+  unsigned long dummy1;
+  unsigned long dummy2;
+  int hammer;
 } profile_t;
 
 /* Address sample */
-typedef struct{
-	unsigned long phy_page;
-	u64 addr;
-	u64 lat;
-	u64 time;
-	unsigned int src;
-	int ld_st;//sample is load or store
-	int cpu;
-}sample_t;
+typedef struct {
+  unsigned long phy_page;
+  u64 addr;
+  u64 lat;
+  u64 time;
+  unsigned int src;
+  int ld_st;  // sample is load or store
+  int cpu;
+} sample_t;
 
 /* for logging */
-struct sample_log{
-	profile_t profile[20];
-	unsigned int record_size;
-	unsigned int sample_total;
-	unsigned int hammer_threshold;
-	int cpu;
+struct sample_log {
+  profile_t profile[20];
+  unsigned int record_size;
+  unsigned int sample_total;
+  unsigned int hammer_threshold;
+  int cpu;
 };
-
-
